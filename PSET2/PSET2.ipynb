{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1A: Initializing the state space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class State:\n",
    "    \n",
    "    def __init__(self, x_position, y_position, heading):\n",
    "        self.x_position = x_position\n",
    "        self.y_position = y_position\n",
    "        self.heading = heading\n",
    "        \n",
    "    def return_current_state(self):\n",
    "        return self.x_position, self.y_position, self.heading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1B: Initializing the action space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Action:\n",
    "    \n",
    "    def __init__(self, move, rotation):\n",
    "        self.move = move\n",
    "        self.rotation = rotation\n",
    "        \n",
    "    def return_action(self):\n",
    "        return self.move, self.rotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1C: Return the probabilty Psa(s') given pe, s, a, s'\n",
    "## 1D: return a next state s' given error probability pe, s, a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### This is the Markov decision process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MarkovDecisionProcess:\n",
    "    \n",
    "    # get the length and the width of the space that we are in\n",
    "    def __init__(self, length, width):\n",
    "        self.length = length\n",
    "        self.width = width\n",
    "    \n",
    "    # check for bound violations\n",
    "    def check_bounds(self, next_x, next_y, current_x, current_y):\n",
    "        if (next_x < 0 or next_x >= self.width):\n",
    "            next_x = current_x\n",
    "        if (next_y < 0 or next_y >= self.length):\n",
    "            next_y = current_y\n",
    "        return next_x, next_y\n",
    "    \n",
    "    #define the directions\n",
    "    left = {8,9,10}\n",
    "    right = {2,3,4}\n",
    "    up = {11,0,1}\n",
    "    down = {5,6,7}\n",
    "    \n",
    "    # find out the next step --> returns next_x, next_y, next_heading\n",
    "    def next_state_compiled(self, state, action):\n",
    "        # get x, y and heading from state\n",
    "        current_x = state.x_position\n",
    "        current_y = state.y_position\n",
    "        current_heading = state.heading\n",
    "        \n",
    "        # get move and rotation from action\n",
    "        move = action.move\n",
    "        rotation = action.rotation\n",
    "        \n",
    "        # check if heading is left, need to only change x and not y\n",
    "        if current_heading in self.left:\n",
    "            next_x = current_x - move\n",
    "            next_y = current_y\n",
    "        \n",
    "        # check if heading is right, need to only change x and not y\n",
    "        elif current_heading in self.right:\n",
    "            next_x = current_x + move\n",
    "            next_y = current_y\n",
    "        \n",
    "        # check if heading is up, need to only change y and not x\n",
    "        elif current_heading in self.up:\n",
    "            next_x = current_x\n",
    "            next_y = current_y + move\n",
    "        \n",
    "        # check if heading is down, need to only change y and not x\n",
    "        else:\n",
    "            next_x = current_x\n",
    "            next_y = current_y - move\n",
    "        \n",
    "        # need to check the bounds\n",
    "        next_x, next_y = self.check_bounds(next_x, next_y, current_x, current_y)\n",
    "        \n",
    "        # finally, need to accomodate for the rotation in heading\n",
    "        next_heading = (current_heading + rotation) % 12\n",
    "        \n",
    "        return next_x, next_y, next_heading\n",
    "    \n",
    "    # find out the next step for the movements that have the prob of error\n",
    "    def next_state_individualized(self, current_x, current_y, current_heading, move, rotation):\n",
    "        # check if heading is left, need to only change x and not y\n",
    "        if current_heading in self.left:\n",
    "            next_x = current_x - move\n",
    "            next_y = current_y\n",
    "        \n",
    "        # check if heading is right, need to only change x and not y\n",
    "        elif current_heading in self.right:\n",
    "            next_x = current_x + move\n",
    "            next_y = current_y\n",
    "        \n",
    "        # check if heading is up, need to only change y and not x\n",
    "        elif current_heading in self.up:\n",
    "            next_x = current_x\n",
    "            next_y = current_y + move\n",
    "        \n",
    "        # check if heading is down, need to only change y and not x\n",
    "        else:\n",
    "            next_x = current_x\n",
    "            next_y = current_y - move\n",
    "        \n",
    "        # need to check the bounds\n",
    "        \n",
    "        next_x, next_y = self.check_bounds(next_x, next_y, current_x, current_y)\n",
    "        \n",
    "        # finally, need to accomodate for the rotation in heading\n",
    "        next_heading = (current_heading + rotation) % 12\n",
    "        \n",
    "        return next_x, next_y, next_heading\n",
    "    \n",
    "    # find out the transition probabilities\n",
    "    def transition_probabilities(self, prob_of_error, action, current_state, next_state):\n",
    "        # get move and rotation from action\n",
    "        move = action.move\n",
    "        rotation = action.rotation\n",
    "        \n",
    "        # if we do not move, current state should equal next state. \n",
    "        if move == 0:\n",
    "            if current_state.return_current_state() == next_state.return_current_state():\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "        \n",
    "        # in the case that we do move, need to account for the error probabilities\n",
    "        \n",
    "        if next_state.return_current_state() == self.next_state_compiled(current_state, action):\n",
    "            return 1-(2*prob_of_error)\n",
    "        else:\n",
    "            current_x = current_state.x_position\n",
    "            current_y = current_state.y_position\n",
    "            current_heading = current_state.heading\n",
    "\n",
    "            # need to check to return the prob of error\n",
    "            if next_state.return_current_state() == self.next_state_individualized(current_x, current_y, (current_heading-1)%12, move, rotation):\n",
    "                return prob_of_error\n",
    "\n",
    "            if next_state.return_current_state() == self.next_state_individualized(current_x, current_y, (current_heading+1)%12, move, rotation):\n",
    "                return prob_of_error\n",
    "        \n",
    "        return 0\n",
    "    \n",
    "    number_of_headings = 12\n",
    "    \n",
    "    # Part 1D\n",
    "    def compute_next_state(self, prob_of_error, current_state, action):\n",
    "        \n",
    "        wrong_states = []\n",
    "        # need to loop through all of the values of x, y and heading\n",
    "        for i in range(self.length):\n",
    "            for j in range(self.width):\n",
    "                for k in range(self.number_of_headings):\n",
    "                    # define the next state\n",
    "                    next_state = State(i,j,k)\n",
    "                    # find the transition probability between current state and the next state\n",
    "                    prob_of_transition = self.transition_probabilities(prob_of_error, action, current_state, next_state)\n",
    "                    # if the probability of transitioning does not equal 0, then only do we proceed. If 0, do not care\n",
    "                    if prob_of_transition != 0:\n",
    "                        # if our transition probabilty is the probability of error, it means that there was an error. Need to keep track of these next state values\n",
    "                        if prob_of_transition == prob_of_error:\n",
    "                            wrong_states.append(next_state)\n",
    "                        else:\n",
    "                            correct_next_state = next_state\n",
    "        # if I choose a random number between 0 and 1 and my number is less than 2*prob_of_error, I move in the wrong direction. If not, I move in the correct direction.\n",
    "        random_number_generated = np.random.uniform(0,1)\n",
    "        if random_number_generated < 2*prob_of_error:\n",
    "            random_index = random.randrange(len(wrong_states))\n",
    "            return wrong_states[random_index]\n",
    "        else:\n",
    "            return correct_next_state\n",
    "    def reward(self, current_state):\n",
    "        # define the lengh and the width\n",
    "        length = width = 6\n",
    "\n",
    "        # get the current x, y positions -- do not care about heading\n",
    "        current_x, current_y, _ = current_state.return_current_state()\n",
    "\n",
    "        # define the rewards\n",
    "        if current_x == 3 and current_y == 4:\n",
    "            reward = 1\n",
    "        elif current_x == 0 or current_x == width-1:\n",
    "            reward = -100\n",
    "        elif current_y == 0 or current_y == length-1:\n",
    "            reward = -100\n",
    "        elif current_x == 2 or current_x == 4:\n",
    "            if current_y == 2 or current_y == 3 or current_y == 4:\n",
    "                reward = -1\n",
    "            else:\n",
    "                reward = 0\n",
    "        else:\n",
    "            reward = 0\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2, 0)\n"
     ]
    }
   ],
   "source": [
    "r = MarkovDecisionProcess(6,6)\n",
    "s = State(1,1,0)\n",
    "f = Action(1,0)\n",
    "\n",
    "print(r.compute_next_state(0.1, s, f).return_current_state())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Write a function that returns the reward R(s) given input s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reward_func(self, current_state):\n",
    "    # define the lengh and the width\n",
    "    length = width = 6\n",
    "\n",
    "    # get the current x, y positions -- do not care about heading\n",
    "    current_x, current_y, _ = current_state.return_current_state()\n",
    "\n",
    "    # define the rewards\n",
    "    if current_x == 3 and current_y == 4:\n",
    "        reward = 1\n",
    "    if current_x == 0 or current_x == width-1:\n",
    "        reward = -100\n",
    "    if current_y == 0 or current_y == height-1:\n",
    "        reward = -100\n",
    "    if current_x == 2 or current_x == 4:\n",
    "        if current_y == 2 or current_y == 3 or current_y == 4:\n",
    "            reward = -1\n",
    "    else:\n",
    "        reward = 0\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3a: Create and populate a matrix/array that stores the action a = pi0(s) prescribed by the initial policy pi0 when indexed by state s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Policy_Matrix:\n",
    "    def __init__(self, given_policy=None):\n",
    "        if given_policy == None:\n",
    "            up = {11, 0, 1}\n",
    "            right = {2, 3, 4}\n",
    "            down = {5, 6, 7}\n",
    "            left = {8, 9, 10}\n",
    "\n",
    "            mat_up    = [[None for x in range(6)] for y in range(6)]\n",
    "            mat_down  = [[None for x in range(6)] for y in range(6)]\n",
    "            mat_left  = [[None for x in range(6)] for y in range(6)]\n",
    "            mat_right = [[None for x in range(6)] for y in range(6)]\n",
    "\n",
    "            rot = 0 # rotation (none-0,left-1,right-2)\n",
    "            mov = 0 # move (none-0,back-1,forward-2)\n",
    "\n",
    "            # populate up matrix\n",
    "            for x in range(6):\n",
    "                for y in range(6):\n",
    "                    # determine rotation\n",
    "                    if (x<3): # goal on right\n",
    "                        rot = 2\n",
    "                    elif (x>3): # goal on left\n",
    "                        rot = 1\n",
    "                    else:\n",
    "                        rot = 0\n",
    "                    # determine move\n",
    "                    if (y<=4):\n",
    "                        mov = 2\n",
    "                    else:\n",
    "                        mov = 1\n",
    "                    # on goal\n",
    "                    if (x==3 and y==4):\n",
    "                        rot = 0\n",
    "                        mov = 0\n",
    "                    mat_up[x][y] = mov, rot\n",
    "\n",
    "            # populate down matrix\n",
    "            for x in range(6):\n",
    "                for y in range(6):\n",
    "                    # determine rotation\n",
    "                    if (x<3): # goal on left\n",
    "                        rot = 1\n",
    "                    elif (x>3): # goal on right\n",
    "                        rot = 2\n",
    "                    else:\n",
    "                        rot = 0\n",
    "                    # determine move\n",
    "                    if (y<=3):\n",
    "                        mov = 1\n",
    "                    else:\n",
    "                        mov = 2\n",
    "                    # on goal\n",
    "                    if (x==3 and y==4):\n",
    "                        rot = 0\n",
    "                        mov = 0\n",
    "                    mat_down[x][y] = mov, rot\n",
    "\n",
    "            # populate right matrix\n",
    "            for x in range(6):\n",
    "                for y in range(6):\n",
    "                    # determine rotation\n",
    "                    if(y>4): # goal on right\n",
    "                        rot = 2\n",
    "                    elif(y<4): # goal on left\n",
    "                        rot = 1\n",
    "                    else:\n",
    "                        rot = 0\n",
    "                    # determine move\n",
    "                    if(x<4):\n",
    "                        mov = 2\n",
    "                    else:\n",
    "                        mov = 1\n",
    "                    # on goal\n",
    "                    if (x==3 and y==4):\n",
    "                        rot = 0\n",
    "                        mov = 0\n",
    "                    mat_right[x][y] = mov, rot\n",
    "\n",
    "            # populate left matrix\n",
    "            for x in range(6):\n",
    "                for y in range(6):\n",
    "                    # determine rotation\n",
    "                    if(y>4): # goal on left\n",
    "                        rot = 1\n",
    "                    elif(y<4): # goal on right\n",
    "                        rot = 2\n",
    "                    else:\n",
    "                        rot = 0\n",
    "                    # determine move\n",
    "                    if(x<3):\n",
    "                        mov = 1\n",
    "                    else:\n",
    "                        mov = 2\n",
    "                    # on goal\n",
    "                    if (x==3 and y==4):\n",
    "                        rot = 0\n",
    "                        mov = 0\n",
    "                    mat_left[x][y] = mov, rot\n",
    "\n",
    "            # matrix for each heading degree\n",
    "            self.pol_mat = [[]]*12\n",
    "            for heading in range(12):\n",
    "                if heading in up:\n",
    "                    self.pol_mat[heading] = mat_up\n",
    "                elif heading in down:\n",
    "                    self.pol_mat[heading] = mat_down\n",
    "                elif heading in left:\n",
    "                    self.pol_mat[heading] = mat_left\n",
    "                else:\n",
    "                    self.pol_mat[heading] = mat_right\n",
    "        else:\n",
    "            self.pol_mat = given_policy\n",
    "            \n",
    "    def policy_action(self, current_state):        \n",
    "        pos_x = current_state.x_position\n",
    "        pos_y = current_state.y_position\n",
    "        heading = current_state.heading\n",
    "        return self.pol_mat[heading][pos_x][pos_y]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pi_0 = Policy_Matrix()\n",
    "#     print(pi_0.pol_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Question 3b: Write a function to generate and plot a trajectory of a robot given policy matrix/array \u0019, initial state s0, and error probability pe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_trajectory(policy, current_state, prob_of_error):\n",
    "    # initialize the trajectory\n",
    "    full_trajectory = []\n",
    "    # get the current x and y position\n",
    "    current_x, current_y, _ = current_state.return_current_state()\n",
    "    # append the current x and y to the trajectory as an array\n",
    "    full_trajectory.append([current_x, current_y])\n",
    "    \n",
    "    # keep looping till we get to the goal\n",
    "    while current_x != 3 or current_y != 4:\n",
    "        move_from_policy, rotation_from_policy = Policy_Matrix().policy_action(current_state)\n",
    "        if move_from_policy == 0:\n",
    "            move = 0\n",
    "        if move_from_policy == 1:\n",
    "            move = -1\n",
    "        if move_from_policy == 2:\n",
    "            move = 1\n",
    "        if rotation_from_policy == 0:\n",
    "            rotation = 0\n",
    "        if rotation_from_policy == 1:\n",
    "            rotation = -1\n",
    "        if rotation_from_policy == 2:\n",
    "            rotation = 1\n",
    "        action = Action(move, rotation)\n",
    "        next_state = MarkovDecisionProcess(6,6).compute_next_state(prob_of_error, current_state, action)\n",
    "        current_x, current_y, _ = next_state.return_current_state()\n",
    "        full_trajectory.append([current_x, current_y])\n",
    "        current_state = next_state\n",
    "        \n",
    "    return full_trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3c:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x109c466d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pi_0 = Policy_Matrix()\n",
    "current_state = State(1,4,6)\n",
    "full_trajectory = compute_trajectory(pi_0, current_state, 0)\n",
    "plt.xlim(0,6)\n",
    "plt.ylim(0,6)\n",
    "for i in range(len(full_trajectory)-1):\n",
    "#     pass\n",
    "    x,y = full_trajectory[i]\n",
    "    x_one_up, y_one_up = full_trajectory[i+1]\n",
    "    plt.arrow(x,y,x_one_up-x,y_one_up-y, head_width=0.1, head_length=0.1,fc='k', ec='k')\n",
    "# plt.arrow(1, 4, 0, -1, head_width=0.05, head_length=0.1, fc='k', ec='k')\n",
    "plt.grid(color='r', linestyle='-', linewidth=0.25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3d:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helped function to calcluate all adjacent states. Need this for both policy and value iteration\n",
    "def calc_adj_states(current_state):\n",
    "    x = current_state.x_position\n",
    "    y = current_state.y_position\n",
    "    h = current_state.heading\n",
    "    \n",
    "    next_poss_h = [h,  (h+1) % 12, (h-1) % 12, (h+2) % 12, (h-2) % 12] # five possible headings\n",
    "    next_poss_x = [x] # first possibility is not moving\n",
    "    next_poss_y = [y] # first possibility is not moving\n",
    "    \n",
    "    # down, up, left, right\n",
    "    if (x-1 >= 0):\n",
    "        next_poss_x.append(x-1)\n",
    "    if (x+1 < 6):\n",
    "        next_poss_x.append(x+1)\n",
    "    if (y-1 >= 0):\n",
    "        next_poss_y.append(y-1)\n",
    "    if (y+1 < 6):\n",
    "        next_poss_y.append(y+1)        \n",
    "    \n",
    "    all_adj_states = []\n",
    "    for x1 in next_poss_x:\n",
    "        for y1 in next_poss_y:\n",
    "            for h1 in next_poss_h:\n",
    "                all_adj_states.append(State(x1,y1,h1))\n",
    "    return all_adj_states\n",
    "\n",
    "def policy_evaluation(initial_poicy, discount_factor):\n",
    "    #start by defining some constants\n",
    "    prob_of_error = 0\n",
    "    difference_between_iterations = -1\n",
    "    \n",
    "    init_V = np.zeros((6, 6, 12)) # initial value matrix\n",
    "    \n",
    "    # need to keep looping till we get to no difference between iterations\n",
    "    while difference_between_iterations != 0:\n",
    "        \n",
    "        current_iteration = np.zeros((6, 6, 12))\n",
    "        # need to loop through all possibilities\n",
    "        for x_pos in range(6):\n",
    "            for y_pos in range(6):\n",
    "                for heading in range(12):\n",
    "                    current_state = State(x_pos, y_pos, heading)\n",
    "                    poss_states = calc_adj_states(current_state)\n",
    "                    \n",
    "                    for next_possible_state in poss_states:\n",
    "                        # need to get the action, probability and reward\n",
    "                        move, rotation = Policy_Matrix().policy_action(current_state)\n",
    "                        action = Action(move, rotation)\n",
    "                        probability_of_move = MarkovDecisionProcess(6,6).transition_probabilities(prob_of_error, action, current_state, next_possible_state)\n",
    "                        current_x, current_y, _ = current_state.return_current_state()\n",
    "#                         print(current_x, current_y)\n",
    "                        reward_for_move = MarkovDecisionProcess(6,6).reward(current_state)\n",
    "                        \n",
    "                        next_possible_x, next_possible_y, next_possible_heading = next_possible_state.return_current_state()\n",
    "                        current_iteration[x_pos, y_pos, heading] += probability_of_move * (reward_for_move+(discount_factor*init_V[next_possible_x, next_possible_y, next_possible_heading]))\n",
    "                        \n",
    "        difference_between_iterations = np.sum(np.abs(current_iteration - init_V))\n",
    "        init_V = current_iteration\n",
    "    \n",
    "    return current_iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3e: What is the value of the trajectory in 3(c)? Use lambda = 0:9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[   0.       0.       0.       0.       0.    -409.51  -343.9   -271.\n",
      "   -190.    -100.    -100.       0.   ]\n",
      "  [   0.       0.       0.       0.       0.    -409.51  -343.9   -271.\n",
      "   -190.    -100.    -100.       0.   ]\n",
      "  [   0.       0.       0.       0.       0.    -409.51  -343.9   -271.\n",
      "   -190.    -100.    -100.       0.   ]\n",
      "  [   0.       0.       0.       0.       0.    -409.51  -343.9   -271.\n",
      "   -190.    -100.    -100.       0.   ]\n",
      "  [-100.    -100.       0.       0.       0.       0.       0.       0.\n",
      "   -1000.    -1000.    -1000.    -190.   ]\n",
      "  [-100.    -100.       0.       0.       0.       0.       0.       0.\n",
      "   -409.51  -343.9   -271.    -190.   ]]\n",
      "\n",
      " [[   0.       0.       0.       0.       0.    -409.51  -343.9   -271.\n",
      "   -190.    -100.    -100.       0.   ]\n",
      "  [   0.       0.       0.       0.       0.    -309.51  -243.9   -171.\n",
      "    -90.       0.       0.       0.   ]\n",
      "  [   0.       0.       0.       0.       0.    -219.51  -153.9    -81.\n",
      "    -90.       0.       0.       0.   ]\n",
      "  [   0.       0.       0.       0.       0.    -138.51   -72.9    -81.\n",
      "    -90.       0.       0.       0.   ]\n",
      "  [   0.       0.       0.       0.       0.       0.       0.       0.\n",
      "   -900.    -900.    -900.       0.   ]\n",
      "  [-100.    -100.       0.       0.       0.       0.       0.       0.\n",
      "   -409.51  -343.9   -271.    -190.   ]]\n",
      "\n",
      " [[   0.       0.       0.       0.       0.    -409.51  -343.9   -271.\n",
      "   -190.    -100.    -100.       0.   ]\n",
      "  [   0.       0.       0.       0.       0.    -309.51  -243.9   -171.\n",
      "      0.       0.       0.       0.   ]\n",
      "  [   0.       0.       0.       0.       0.    -220.51  -154.9     -1.\n",
      "     -1.      -1.      -1.       0.   ]\n",
      "  [   0.       0.       0.       0.       0.    -140.41    -1.9     -1.9\n",
      "     -1.      -1.      -1.       0.   ]\n",
      "  [  -1.      -1.       0.       0.       0.       0.       0.       0.\n",
      "   -811.    -811.    -811.      -1.9  ]\n",
      "  [-100.    -100.       0.       0.       0.       0.       0.       0.\n",
      "   -409.51  -343.9   -271.    -190.   ]]\n",
      "\n",
      " [[   0.       0.       0.       0.       0.    -1000.    -1000.    -1000.\n",
      "      0.       0.       0.       0.   ]\n",
      "  [   0.       0.       0.       0.       0.    -900.    -900.    -900.\n",
      "      0.       0.       0.       0.   ]\n",
      "  [   0.       0.       0.       0.       0.    -810.    -810.    -810.\n",
      "      0.       0.       0.       0.   ]\n",
      "  [   0.       0.       0.       0.       0.    -729.    -729.    -729.\n",
      "      0.       0.       0.       0.   ]\n",
      "  [  10.      10.      10.      10.      10.      10.      10.      10.\n",
      "     10.      10.      10.      10.   ]\n",
      "  [-1000.    -1000.       0.       0.       0.       0.       0.       0.\n",
      "      0.       0.       0.    -1000.   ]]\n",
      "\n",
      " [[   0.       0.    -409.51  -343.9   -271.    -190.    -100.    -100.\n",
      "      0.       0.       0.       0.   ]\n",
      "  [   0.       0.    -309.51  -243.9   -171.     -90.       0.       0.\n",
      "      0.       0.       0.       0.   ]\n",
      "  [   0.       0.    -310.51  -244.9   -172.      -1.      -1.      -1.\n",
      "      0.       0.       0.       0.   ]\n",
      "  [   0.       0.    -310.51  -244.9   -172.      -1.9     -1.      -1.\n",
      "      0.       0.       0.       0.   ]\n",
      "  [-731.71  -811.9   -901.    -901.    -901.       0.       0.       0.\n",
      "      0.       0.       0.    -659.539]\n",
      "  [-343.9   -271.    -190.    -100.    -100.       0.       0.       0.\n",
      "      0.       0.       0.    -409.51 ]]\n",
      "\n",
      " [[   0.       0.    -409.51  -343.9   -271.    -190.    -100.    -100.\n",
      "      0.       0.       0.       0.   ]\n",
      "  [   0.       0.    -409.51  -343.9   -271.    -190.    -100.    -100.\n",
      "      0.       0.       0.       0.   ]\n",
      "  [   0.       0.    -409.51  -343.9   -271.    -190.    -100.    -100.\n",
      "      0.       0.       0.       0.   ]\n",
      "  [   0.       0.    -409.51  -343.9   -271.    -190.    -100.    -100.\n",
      "      0.       0.       0.       0.   ]\n",
      "  [-1000.    -1000.    -1000.    -1000.    -1000.       0.       0.       0.\n",
      "      0.       0.       0.    -1000.   ]\n",
      "  [-343.9   -271.    -190.    -100.    -100.       0.       0.       0.\n",
      "      0.       0.       0.    -409.51 ]]]\n"
     ]
    }
   ],
   "source": [
    "pi_0 = Policy_Matrix()\n",
    "value = policy_evaluation(pi_0, 0.9)\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value[1][4][6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4a:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "action_space = [(-1, -1), (-1, 0), (-1, 1), (0, 0), (1, -1), (1, 0), (1, 1)]\n",
    "\n",
    "def value_iteration(discount):\n",
    "    prev_V = np.zeros((12, 6, 6)) # initial value matrix\n",
    "    new_pol_mat = [[[None for l in range(6)] for w in range(6)] for h in range(12)] # new policy matrix\n",
    "    err_p = 0 # error probability\n",
    "    conv = 0 # convergence boolean\n",
    "    \n",
    "    while (conv != 1):\n",
    "        new_V = np.zeros((12, 6, 6)) # new value matrix\n",
    "        for x_pos in range(6):\n",
    "            for y_pos in range(6):\n",
    "                for heading in range(12):\n",
    "                    current_state = State(x_pos, y_pos, heading)\n",
    "                    poss_states = calc_adj_states(current_state)\n",
    "                    best_action = None\n",
    "                    print(heading, x_pos, y_pos)\n",
    "                    max_action_val = float('-inf')\n",
    "                    for act in action_space:\n",
    "                        action = Action(act[0], act[1]) # create action object for transition probability calculation\n",
    "                        action_val = 0\n",
    "                        for next_state in poss_states:\n",
    "                            x_, y_, h_ = next_state.return_current_state()\n",
    "                            mdp = MarkovDecisionProcess(6,6)\n",
    "                            \n",
    "                            action_val += mdp.transition_probabilities(err_p, action, current_state, next_state) * (mdp.reward(current_state) + discount*prev_V[h_][x_][y_])\n",
    "                        if (action_val > max_action_val):\n",
    "                            max_action_val = action_val\n",
    "                            best_action = action\n",
    "                    # update policy matrix and new value matrix\n",
    "                    new_pol_mat[heading][x_pos][y_pos] = best_action\n",
    "                    new_V[heading][x_pos][y_pos] = max_action_val\n",
    "        \n",
    "        # check if convergence occurs\n",
    "        if(np.equal(new_V, prev_V)):\n",
    "            break\n",
    "        # if not, update value matrix\n",
    "        prev_v = new_V\n",
    "    # create final policy\n",
    "    new_pol = Policy(new_pol_mat)\n",
    "    return new_pol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing value iteration function\n",
    "discount = 0.9\n",
    "# opt_pol = value_iteration(discount)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
