{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1A: Initializing the state space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State:\n",
    "    \n",
    "    def __init__(self, x_position, y_position, heading):\n",
    "        self.x_position = x_position\n",
    "        self.y_position = y_position\n",
    "        self.heading = heading\n",
    "        \n",
    "    def return_current_state(self):\n",
    "        return self.x_position, self.y_position, self.heading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1B: Initializing the action space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Action:\n",
    "    \n",
    "    def __init__(self, move, rotation):\n",
    "        self.move = move\n",
    "        self.rotation = rotation\n",
    "        \n",
    "    def return_action(self):\n",
    "        return self.move, self.rotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1C: Return the probabilty Psa(s') given pe, s, a, s'\n",
    "## 1D: return a next state s' given error probability pe, s, a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### This is the Markov decision process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkovDecisionProcess:\n",
    "    \n",
    "    # get the length and the width of the space that we are in\n",
    "    def __init__(self, length, width):\n",
    "        self.length = length\n",
    "        self.width = width\n",
    "    \n",
    "    # check for bound violations\n",
    "    def check_bounds(self, next_x, next_y, current_x, current_y):\n",
    "        if (next_x < 0 or next_x >= self.width):\n",
    "            next_x = current_x\n",
    "        if (next_y < 0 or next_y >= self.length):\n",
    "            next_y = current_y\n",
    "        return next_x, next_y\n",
    "    \n",
    "    #define the directions\n",
    "    left = {8,9,10}\n",
    "    right = {2,3,4}\n",
    "    up = {11,0,1}\n",
    "    down = {5,6,7}\n",
    "    \n",
    "    # find out the next step --> returns next_x, next_y, next_heading\n",
    "    def next_state_compiled(self, state, action):\n",
    "        # get x, y and heading from state\n",
    "        current_x = state.x_position\n",
    "        current_y = state.y_position\n",
    "        current_heading = state.heading\n",
    "        \n",
    "        # get move and rotation from action\n",
    "        move = action.move\n",
    "        rotation = action.rotation\n",
    "        \n",
    "        # check if heading is left, need to only change x and not y\n",
    "        if current_heading in self.left:\n",
    "            next_x = current_x - move\n",
    "            next_y = current_y\n",
    "        \n",
    "        # check if heading is right, need to only change x and not y\n",
    "        elif current_heading in self.right:\n",
    "            next_x = current_x + move\n",
    "            next_y = current_y\n",
    "        \n",
    "        # check if heading is up, need to only change y and not x\n",
    "        elif current_heading in self.up:\n",
    "            next_x = current_x\n",
    "            next_y = current_y + move\n",
    "        \n",
    "        # check if heading is down, need to only change y and not x\n",
    "        else:\n",
    "            next_x = current_x\n",
    "            next_y = current_y - move\n",
    "        \n",
    "        # need to check the bounds\n",
    "        next_x, next_y = self.check_bounds(next_x, next_y, current_x, current_y)\n",
    "        \n",
    "        # finally, need to accomodate for the rotation in heading\n",
    "        next_heading = (current_heading + rotation) % 12\n",
    "        \n",
    "        return next_x, next_y, next_heading\n",
    "    \n",
    "    # find out the next step for the movements that have the prob of error\n",
    "    def next_state_individualized(self, current_x, current_y, current_heading, move, rotation):\n",
    "        # check if heading is left, need to only change x and not y\n",
    "        if current_heading in self.left:\n",
    "            next_x = current_x - move\n",
    "            next_y = current_y\n",
    "        \n",
    "        # check if heading is right, need to only change x and not y\n",
    "        elif current_heading in self.right:\n",
    "            next_x = current_x + move\n",
    "            next_y = current_y\n",
    "        \n",
    "        # check if heading is up, need to only change y and not x\n",
    "        elif current_heading in self.up:\n",
    "            next_x = current_x\n",
    "            next_y = current_y + move\n",
    "        \n",
    "        # check if heading is down, need to only change y and not x\n",
    "        else:\n",
    "            next_x = current_x\n",
    "            next_y = current_y - move\n",
    "        \n",
    "        # need to check the bounds\n",
    "        \n",
    "        next_x, next_y = self.check_bounds(next_x, next_y, current_x, current_y)\n",
    "        \n",
    "        # finally, need to accomodate for the rotation in heading\n",
    "        next_heading = (current_heading + rotation) % 12\n",
    "        \n",
    "        return next_x, next_y, next_heading\n",
    "    \n",
    "    # find out the transition probabilities\n",
    "    def transition_probabilities(self, prob_of_error, action, current_state, next_state):\n",
    "        # get move and rotation from action\n",
    "        move = action.move\n",
    "        rotation = action.rotation\n",
    "        \n",
    "        # if we do not move, current state should equal next state. \n",
    "        if move == 0:\n",
    "            if current_state.return_current_state() == next_state.return_current_state():\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "        \n",
    "        # in the case that we do move, need to account for the error probabilities\n",
    "        \n",
    "        if next_state.return_current_state() == self.next_state_compiled(current_state, action):\n",
    "            return 1-(2*prob_of_error)\n",
    "        else:\n",
    "            current_x = current_state.x_position\n",
    "            current_y = current_state.y_position\n",
    "            current_heading = current_state.heading\n",
    "\n",
    "            # need to check to return the prob of error\n",
    "            if next_state.return_current_state() == self.next_state_individualized(current_x, current_y, (current_heading-1)%12, move, rotation):\n",
    "                return prob_of_error\n",
    "\n",
    "            if next_state.return_current_state() == self.next_state_individualized(current_x, current_y, (current_heading+1)%12, move, rotation):\n",
    "                return prob_of_error\n",
    "        \n",
    "        return 0\n",
    "    \n",
    "    number_of_headings = 12\n",
    "    \n",
    "    # Part 1D\n",
    "    def compute_next_state(self, prob_of_error, current_state, action):\n",
    "        \n",
    "        wrong_states = []\n",
    "        # need to loop through all of the values of x, y and heading\n",
    "        for i in range(self.length):\n",
    "            for j in range(self.width):\n",
    "                for k in range(self.number_of_headings):\n",
    "                    # define the next state\n",
    "                    next_state = State(i,j,k)\n",
    "                    # find the transition probability between current state and the next state\n",
    "                    prob_of_transition = self.transition_probabilities(prob_of_error, action, current_state, next_state)\n",
    "                    # if the probability of transitioning does not equal 0, then only do we proceed. If 0, do not care\n",
    "                    if prob_of_transition != 0:\n",
    "                        # if our transition probabilty is the probability of error, it means that there was an error. Need to keep track of these next state values\n",
    "                        if prob_of_transition == prob_of_error:\n",
    "                            wrong_states.append(next_state)\n",
    "                        else:\n",
    "                            correct_next_state = next_state\n",
    "        # if I choose a random number between 0 and 1 and my number is less than 2*prob_of_error, I move in the wrong direction. If not, I move in the correct direction.\n",
    "        random_number_generated = np.random.uniform(0,1)\n",
    "        if random_number_generated < 2*prob_of_error:\n",
    "            random_index = random.randrange(len(wrong_states))\n",
    "            return wrong_states[random_index]\n",
    "        else:\n",
    "            return correct_next_state\n",
    "    def reward(self, current_state):\n",
    "        # define the lengh and the width\n",
    "        length = width = 6\n",
    "\n",
    "        # get the current x, y positions -- do not care about heading\n",
    "        current_x, current_y, _ = current_state.return_current_state()\n",
    "\n",
    "        # define the rewards\n",
    "        if current_x == 3 and current_y == 4:\n",
    "            reward = 1\n",
    "        elif current_x == 0 or current_x == width-1:\n",
    "            reward = -100\n",
    "        elif current_y == 0 or current_y == length-1:\n",
    "            reward = -100\n",
    "        elif current_x == 2 or current_x == 4:\n",
    "            if current_y == 2 or current_y == 3 or current_y == 4:\n",
    "                reward = -10\n",
    "            else:\n",
    "                reward = 0\n",
    "        else:\n",
    "            reward = 0\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2, 0)\n"
     ]
    }
   ],
   "source": [
    "r = MarkovDecisionProcess(6,6)\n",
    "s = State(1,1,0)\n",
    "f = Action(1,0)\n",
    "\n",
    "print(r.compute_next_state(0.1, s, f).return_current_state())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Write a function that returns the reward R(s) given input s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_func(self, current_state):\n",
    "    # define the lengh and the width\n",
    "    length = width = 6\n",
    "\n",
    "    # get the current x, y positions -- do not care about heading\n",
    "    current_x, current_y, _ = current_state.return_current_state()\n",
    "\n",
    "    # define the rewards\n",
    "    if current_x == 3 and current_y == 4:\n",
    "        reward = 1\n",
    "    if current_x == 0 or current_x == width-1:\n",
    "        reward = -100\n",
    "    if current_y == 0 or current_y == height-1:\n",
    "        reward = -100\n",
    "    if current_x == 2 or current_x == 4:\n",
    "        if current_y == 2 or current_y == 3 or current_y == 4:\n",
    "            reward = -1\n",
    "    else:\n",
    "        reward = 0\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3a: Create and populate a matrix/array that stores the action a = pi0(s) prescribed by the initial policy pi0 when indexed by state s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy_Matrix:\n",
    "    def __init__(self, given_policy=None):\n",
    "        if given_policy == None:\n",
    "            up = {11, 0, 1}\n",
    "            right = {2, 3, 4}\n",
    "            down = {5, 6, 7}\n",
    "            left = {8, 9, 10}\n",
    "\n",
    "            mat_up    = [[None for x in range(6)] for y in range(6)]\n",
    "            mat_down  = [[None for x in range(6)] for y in range(6)]\n",
    "            mat_left  = [[None for x in range(6)] for y in range(6)]\n",
    "            mat_right = [[None for x in range(6)] for y in range(6)]\n",
    "\n",
    "            rot = 0 # rotation (none-0,left--1,right-1)\n",
    "            mov = 0 # move (none-0,back--1,forward-1)\n",
    "\n",
    "            # populate up matrix\n",
    "            for x in range(6):\n",
    "                for y in range(6):\n",
    "                    # determine rotation\n",
    "                    if (x<3): # goal on right\n",
    "                        rot = 1\n",
    "                    elif (x>3): # goal on left\n",
    "                        rot = -1\n",
    "                    else:\n",
    "                        rot = 0\n",
    "                    # determine move\n",
    "                    if (y<=4):\n",
    "                        mov = 1\n",
    "                    else:\n",
    "                        mov = -1\n",
    "                    # on goal\n",
    "                    if (x==3 and y==4):\n",
    "                        rot = 0\n",
    "                        mov = 0\n",
    "                    mat_up[x][y] = mov, rot\n",
    "\n",
    "            # populate down matrix\n",
    "            for x in range(6):\n",
    "                for y in range(6):\n",
    "                    # determine rotation\n",
    "                    if (x<3): # goal on left\n",
    "                        rot = -1\n",
    "                    elif (x>3): # goal on right\n",
    "                        rot = 1\n",
    "                    else:\n",
    "                        rot = 0\n",
    "                    # determine move\n",
    "                    if (y<=3):\n",
    "                        mov = -1\n",
    "                    else:\n",
    "                        mov = 1\n",
    "                    # on goal\n",
    "                    if (x==3 and y==4):\n",
    "                        rot = 0\n",
    "                        mov = 0\n",
    "                    mat_down[x][y] = mov, rot\n",
    "\n",
    "            # populate right matrix\n",
    "            for x in range(6):\n",
    "                for y in range(6):\n",
    "                    # determine rotation\n",
    "                    if(y>4): # goal on right\n",
    "                        rot = 1\n",
    "                    elif(y<4): # goal on left\n",
    "                        rot = -1\n",
    "                    else:\n",
    "                        rot = 0\n",
    "                    # determine move\n",
    "                    if(x<4):\n",
    "                        mov = 1\n",
    "                    else:\n",
    "                        mov = -1\n",
    "                    # on goal\n",
    "                    if (x==3 and y==4):\n",
    "                        rot = 0\n",
    "                        mov = 0\n",
    "                    mat_right[x][y] = mov, rot\n",
    "\n",
    "            # populate left matrix\n",
    "            for x in range(6):\n",
    "                for y in range(6):\n",
    "                    # determine rotation\n",
    "                    if(y>4): # goal on left\n",
    "                        rot = -1\n",
    "                    elif(y<4): # goal on right\n",
    "                        rot = 1\n",
    "                    else:\n",
    "                        rot = 0\n",
    "                    # determine move\n",
    "                    if(x<3):\n",
    "                        mov = -1\n",
    "                    else:\n",
    "                        mov = 1\n",
    "                    # on goal\n",
    "                    if (x==3 and y==4):\n",
    "                        rot = 0\n",
    "                        mov = 0\n",
    "                    mat_left[x][y] = mov, rot\n",
    "\n",
    "            # matrix for each heading degree\n",
    "            self.pol_mat = [[]]*12\n",
    "            for heading in range(12):\n",
    "                if heading in up:\n",
    "                    self.pol_mat[heading] = mat_up\n",
    "                elif heading in down:\n",
    "                    self.pol_mat[heading] = mat_down\n",
    "                elif heading in left:\n",
    "                    self.pol_mat[heading] = mat_left\n",
    "                else:\n",
    "                    self.pol_mat[heading] = mat_right\n",
    "        else:\n",
    "            self.pol_mat = given_policy\n",
    "            \n",
    "    def policy_action(self, current_state):        \n",
    "        pos_x = current_state.x_position\n",
    "        pos_y = current_state.y_position\n",
    "        heading = current_state.heading\n",
    "        return self.pol_mat[heading][pos_x][pos_y]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pi_0 = Policy_Matrix()\n",
    "#     print(pi_0.pol_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Question 3b: Write a function to generate and plot a trajectory of a robot given policy matrix/array \u0019, initial state s0, and error probability pe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_trajectory(policy, current_state, prob_of_error):\n",
    "    # initialize the trajectory\n",
    "    full_trajectory = []\n",
    "    # get the current x and y position\n",
    "    current_x, current_y, _ = current_state.return_current_state()\n",
    "    # append the current x and y to the trajectory as an array\n",
    "    full_trajectory.append([current_x, current_y])\n",
    "    \n",
    "    # keep looping till we get to the goal\n",
    "    while current_x != 3 or current_y != 4:\n",
    "        move_from_policy, rotation_from_policy = policy.policy_action(current_state)\n",
    "#         if move_from_policy == 0:\n",
    "#             move = 0\n",
    "#         if move_from_policy == 1:\n",
    "#             move = -1\n",
    "#         if move_from_policy == 2:\n",
    "#             move = 1\n",
    "#         if rotation_from_policy == 0:\n",
    "#             rotation = 0\n",
    "#         if rotation_from_policy == 1:\n",
    "#             rotation = -1\n",
    "#         if rotation_from_policy == 2:\n",
    "#             rotation = 1\n",
    "        action = Action(move_from_policy, rotation_from_policy)\n",
    "        next_state = MarkovDecisionProcess(6,6).compute_next_state(prob_of_error, current_state, action)\n",
    "        current_x, current_y, _ = next_state.return_current_state()\n",
    "        full_trajectory.append([current_x, current_y])\n",
    "        current_state = next_state\n",
    "        \n",
    "    return full_trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3c:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcbc7232290>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pi_0 = Policy_Matrix()\n",
    "current_state = State(1,4,6)\n",
    "full_trajectory = compute_trajectory(pi_0, current_state, 0)\n",
    "plt.xlim(0,6)\n",
    "plt.ylim(0,6)\n",
    "for i in range(len(full_trajectory)-1):\n",
    "#     pass\n",
    "    x,y = full_trajectory[i]\n",
    "    x_one_up, y_one_up = full_trajectory[i+1]\n",
    "    plt.arrow(x,y,x_one_up-x,y_one_up-y, head_width=0.1, head_length=0.1,fc='k', ec='k')\n",
    "# plt.arrow(1, 4, 0, -1, head_width=0.05, head_length=0.1, fc='k', ec='k')\n",
    "plt.grid(color='r', linestyle='-', linewidth=0.25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3d:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helped function to calcluate all adjacent states. Need this for both policy and value iteration\n",
    "def calc_adj_states(current_state):\n",
    "    x = current_state.x_position\n",
    "    y = current_state.y_position\n",
    "    h = current_state.heading\n",
    "    \n",
    "    next_poss_h = [h,  (h+1) % 12, (h-1) % 12, (h+2) % 12, (h-2) % 12] # five possible headings\n",
    "    next_poss_x = [x] # first possibility is not moving\n",
    "    next_poss_y = [y] # first possibility is not moving\n",
    "    \n",
    "    # down, up, left, right\n",
    "    if (x-1 >= 0):\n",
    "        next_poss_x.append(x-1)\n",
    "    if (x+1 < 6):\n",
    "        next_poss_x.append(x+1)\n",
    "    if (y-1 >= 0):\n",
    "        next_poss_y.append(y-1)\n",
    "    if (y+1 < 6):\n",
    "        next_poss_y.append(y+1)        \n",
    "    \n",
    "    all_adj_states = []\n",
    "    for x1 in next_poss_x:\n",
    "        for y1 in next_poss_y:\n",
    "            for h1 in next_poss_h:\n",
    "                all_adj_states.append(State(x1,y1,h1))\n",
    "    return all_adj_states\n",
    "\n",
    "def policy_evaluation(initial_poicy, discount_factor):\n",
    "    #start by defining some constants\n",
    "    prob_of_error = 0\n",
    "    difference_between_iterations = -1\n",
    "    \n",
    "    init_V = np.zeros((6, 6, 12)) # initial value matrix\n",
    "    \n",
    "    # need to keep looping till we get to no difference between iterations\n",
    "    while difference_between_iterations != 0:\n",
    "        \n",
    "        current_iteration = np.zeros((6, 6, 12))\n",
    "        # need to loop through all possibilities\n",
    "        for x_pos in range(6):\n",
    "            for y_pos in range(6):\n",
    "                for heading in range(12):\n",
    "                    current_state = State(x_pos, y_pos, heading)\n",
    "                    poss_states = calc_adj_states(current_state)\n",
    "                    \n",
    "                    for next_possible_state in poss_states:\n",
    "                        # need to get the action, probability and reward\n",
    "                        move_from_policy, rotation_from_policy = Policy_Matrix().policy_action(current_state)\n",
    "#                         if move_from_policy == 0:\n",
    "#                             move = 0\n",
    "#                         if move_from_policy == 1:\n",
    "#                             move = -1\n",
    "#                         if move_from_policy == 2:\n",
    "#                             move = 1\n",
    "#                         if rotation_from_policy == 0:\n",
    "#                             rotation = 0\n",
    "#                         if rotation_from_policy == 1:\n",
    "#                             rotation = -1\n",
    "#                         if rotation_from_policy == 2:\n",
    "#                             rotation = 1\n",
    "                        action = Action(move_from_policy, rotation_from_policy)\n",
    "                        probability_of_move = MarkovDecisionProcess(6,6).transition_probabilities(prob_of_error, action, current_state, next_possible_state)\n",
    "                        current_x, current_y, _ = current_state.return_current_state()\n",
    "#                         print(current_x, current_y)\n",
    "                        reward_for_move = MarkovDecisionProcess(6,6).reward(current_state)\n",
    "                        \n",
    "                        next_possible_x, next_possible_y, next_possible_heading = next_possible_state.return_current_state()\n",
    "                        current_iteration[x_pos, y_pos, heading] += probability_of_move * (reward_for_move+(discount_factor*init_V[next_possible_x, next_possible_y, next_possible_heading]))\n",
    "                        \n",
    "        difference_between_iterations = np.sum(np.abs(current_iteration - init_V))\n",
    "        init_V = current_iteration\n",
    "    \n",
    "    return current_iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3e: What is the value of the trajectory in 3(c)? Use lambda = 0:9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-267.338962   -186.463621   -185.873131   -266.217031   -339.117031\n",
      "   -185.217031   -266.873131   -339.707521   -339.117031   -267.463621\n",
      "   -186.338962   -339.648472  ]\n",
      "  [-266.27608    -185.93218     -96.07069     -95.41459     -94.68559\n",
      "   -185.41459    -266.34169    -339.17608     -94.68559     -96.66118\n",
      "    -95.27608    -339.17608   ]\n",
      "  [-265.7512     -184.7512      -95.4802      -95.6341      -94.9051\n",
      "   -184.8241     -265.7512     -339.707521    -94.9051     -301.39040611\n",
      "   -323.66125018 -516.920131  ]\n",
      "  [-463.24459    -184.168       -94.168       -94.978       -94.249\n",
      "   -184.168      -266.34169    -339.17608     -94.249      -162.42508\n",
      "   -168.17608    -339.17608   ]\n",
      "  [-265.7512     -403.6051      -93.52        -93.52        -93.52\n",
      "   -184.8241     -265.7512     -339.707521    -93.52        -93.52        -93.52\n",
      "   -516.920131  ]\n",
      "  [-463.24459    -184.168      -337.339      -265.168      -184.168\n",
      "   -184.168      -266.34169    -339.17608    -185.27608    -267.66118\n",
      "   -337.339      -339.17608   ]]\n",
      "\n",
      " [[ -96.07069     -95.41459    -184.68559    -265.68559    -340.7172589\n",
      "    -97.2955558  -247.54260605 -141.61319499 -340.7172589  -265.68559\n",
      "   -186.07069     -95.93218   ]\n",
      "  [   4.5198        4.3659        5.0949        5.9049        3.004938\n",
      "   -163.93622895  -46.23688332    5.2488        3.004938      5.9049\n",
      "      3.7098        5.2488    ]\n",
      "  [   5.832         5.022         4.851         5.661      -182.1513655\n",
      "    -51.3743148     5.832       -41.61319499 -182.1513655     5.661\n",
      "   -223.7671179  -248.5125002 ]\n",
      "  [-276.12500022    6.48          5.58          6.39        -57.082572\n",
      "      6.48        -46.23688332    5.2488      -57.082572      6.39\n",
      "    -69.3612      -75.7512    ]\n",
      "  [ -84.168      -306.8055558     7.2           7.2           7.2\n",
      "    -51.3743148     5.832       -41.61319499    7.2           7.2           7.2\n",
      "   -338.5125002 ]\n",
      "  [-376.12500022  -93.52       -340.895062   -263.71       -183.52        -93.52\n",
      "   -146.23688332  -94.7512     -186.2902     -263.71       -340.895062\n",
      "   -175.7512    ]]\n",
      "\n",
      " [[ -94.9051      -94.0951     -184.0951     -267.463621   -339.117031\n",
      "    -95.217031    -96.683131    -97.346521   -339.117031   -267.463621\n",
      "   -184.0951      -95.6341    ]\n",
      "  [   4.851         5.661         6.561         3.33882       5.31441\n",
      "      3.68541       2.94831       4.122         5.31441       3.33882\n",
      "      6.561         4.122     ]\n",
      "  [   4.58          5.39          6.29       -202.39040611    4.0949\n",
      "      3.2759        4.58          0.753479      4.0949     -202.39040611\n",
      "      6.29       -248.630131  ]\n",
      "  [-275.14459       6.2           7.1         -63.42508       4.751         6.2\n",
      "      1.94831       3.122         4.751       -63.42508       7.1         -77.068     ]\n",
      "  [ -84.52       -304.6051        8.            8.            8.            3.2759\n",
      "      4.58          0.753479      8.            8.            8.\n",
      "   -337.730131  ]\n",
      "  [-374.14459     -92.8        -337.339      -267.66118    -181.9         -92.8\n",
      "    -97.05169     -95.878      -181.9        -267.66118    -337.339\n",
      "   -176.068     ]]\n",
      "\n",
      " [[ -93.439       -93.439      -186.07069    -265.68559    -340.7172589\n",
      "    -93.439       -93.439       -93.439      -340.7172589  -265.68559\n",
      "   -186.07069     -93.439     ]\n",
      "  [   7.29          7.29          3.7098        5.9049        3.004938\n",
      "      7.29          7.29          7.29          3.004938      5.9049\n",
      "      3.7098        7.29      ]\n",
      "  [   8.1           8.1        -223.7671179     5.661      -182.1513655\n",
      "      8.1           8.1           8.1        -182.1513655     5.661\n",
      "   -223.7671179     8.1       ]\n",
      "  [   9.            9.          -69.3612        6.39        -57.082572\n",
      "      9.            9.            9.          -57.082572      6.39\n",
      "    -69.3612        9.        ]\n",
      "  [  10.           10.           10.           10.           10.           10.\n",
      "     10.           10.           10.           10.           10.           10.        ]\n",
      "  [ -91.          -91.         -340.895062   -263.71       -186.2902      -91.\n",
      "    -91.          -91.         -186.2902     -263.71       -340.895062\n",
      "    -91.        ]]\n",
      "\n",
      " [[ -94.9051      -95.6341     -184.0951     -267.463621   -339.117031\n",
      "    -97.346521    -96.683131    -95.217031   -339.117031   -267.463621\n",
      "   -184.0951      -94.0951    ]\n",
      "  [   4.851         4.122         6.561         3.33882       5.31441\n",
      "      4.122         2.94831       3.68541       5.31441       3.33882\n",
      "      6.561         5.661     ]\n",
      "  [   4.58       -248.630131      6.29       -202.39040611    4.0949\n",
      "      0.753479      4.58          3.2759        4.0949     -202.39040611\n",
      "      6.29          5.39      ]\n",
      "  [-275.14459     -77.068         7.1         -63.42508       4.751\n",
      "      3.122         1.94831       6.2           4.751       -63.42508\n",
      "      7.1           6.2       ]\n",
      "  [ -84.52       -337.730131      8.            8.            8.\n",
      "      0.753479      4.58          3.2759        8.            8.            8.\n",
      "   -304.6051    ]\n",
      "  [-374.14459    -176.068      -337.339      -267.66118    -181.9         -95.878\n",
      "    -97.05169     -92.8        -181.9        -267.66118    -337.339       -92.8       ]]\n",
      "\n",
      " [[-267.07069    -339.83218    -186.07069    -265.68559    -340.7172589\n",
      "   -385.51319499 -418.54260605 -187.2955558  -340.7172589  -265.68559\n",
      "   -184.68559    -185.41459   ]\n",
      "  [-266.4802     -338.6512      -96.2902      -94.0951      -96.995062\n",
      "   -338.6512     -317.23688332 -353.93622895  -96.995062    -94.0951\n",
      "    -94.9051     -185.6341    ]\n",
      "  [-265.168      -519.5125002  -323.7671179   -94.339      -282.1513655\n",
      "   -385.51319499 -265.168      -241.3743148  -282.1513655   -94.339\n",
      "    -95.149      -184.978     ]\n",
      "  [-466.12500022 -338.6512     -169.3612      -93.61       -157.082572\n",
      "   -338.6512     -317.23688332 -183.52       -157.082572    -93.61        -94.42\n",
      "   -183.52      ]\n",
      "  [-265.168      -519.5125002   -92.8         -92.8         -92.8\n",
      "   -385.51319499 -265.168      -241.3743148   -92.8         -92.8         -92.8\n",
      "   -406.8055558 ]\n",
      "  [-466.12500022 -338.6512     -340.895062   -263.71       -186.2902\n",
      "   -338.6512     -317.23688332 -183.52       -183.52       -263.71\n",
      "   -340.895062   -183.52      ]]]\n"
     ]
    }
   ],
   "source": [
    "pi_0 = Policy_Matrix()\n",
    "value = policy_evaluation(pi_0, 0.9)\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.8319999999999963"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value[1][4][6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3f: Write a function that returns a matrix/array A giving the optimal policy given a one-step lookahead on value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def look_ahead(value):\n",
    "    action_space = [(-1, -1), (-1, 0), (-1, 1), (0, 0), (1, -1), (1, 0), (1, 1)]\n",
    "    init_policy = [[[0 for x in range(6)]for y in range(6)]for heading in range(12)]\n",
    "    prob_of_error = 0\n",
    "    # same setup as the last quesiton, iterating through current state and finding next possible states\n",
    "    for x_pos in range(6):\n",
    "            for y_pos in range(6):\n",
    "                for heading in range(12):\n",
    "                    current_state = State(x_pos, y_pos, heading)\n",
    "                    poss_states = calc_adj_states(current_state)\n",
    "                    best_action = None\n",
    "                    max_action_val = float('-inf')\n",
    "                    for act in action_space:\n",
    "                        action = Action(act[0], act[1]) # create action object for transition probability calculation\n",
    "                        action_val = 0\n",
    "                        for next_state in poss_states:\n",
    "                            x_next, y_next, heading_next = next_state.return_current_state()\n",
    "                            mdp = MarkovDecisionProcess(6,6)\n",
    "                            action_val += mdp.transition_probabilities(prob_of_error, action, current_state, next_state) * value[x_next][y_next][heading_next]\n",
    "                        if (action_val > max_action_val):\n",
    "                            max_action_val = action_val\n",
    "                            best_action = action\n",
    "                    init_policy[heading][y][x] = best_action\n",
    "    finalized_policy = Policy_Matrix(given_policy=init_policy)\n",
    "    return finalized_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalized_policy = look_ahead(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3g: Combine 3f and 3d to returning optimal policy with optimal value V."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(starting_policy, discount_factor):\n",
    "        prev_value = policy_evaluation(starting_policy, discount_factor)\n",
    "        prev_policy_maximized = look_ahead(prev_value)\n",
    "        conv = 0\n",
    "        \n",
    "        while conv != 1:\n",
    "            current_value = policy_evaluation(prev_policy_maximized, discount_factor)\n",
    "            current_policy_maximized = look_ahead(current_value)\n",
    "            if np.array_equal(current_value, prev_value):\n",
    "                conv = 1\n",
    "            \n",
    "            prev_value = current_value\n",
    "            prev_policy_maximized = current_policy_maximized\n",
    "        return current_value, current_policy_maximized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3h:recompute and plot the trajectory and value of the robot described in 3(c) under the optimal policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-5b32ab2f8213>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcurrent_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdiscount_factor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmaxed_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxed_policy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpi_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscount_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mfull_trajectory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_trajectory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxed_policy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-565f255b0e0a>\u001b[0m in \u001b[0;36mpolicy_iteration\u001b[0;34m(starting_policy, discount_factor)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpolicy_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstarting_policy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscount_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m         \u001b[0mprev_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstarting_policy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscount_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0mprev_policy_maximized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlook_ahead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mconv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-820c568d562c>\u001b[0m in \u001b[0;36mpolicy_evaluation\u001b[0;34m(initial_poicy, discount_factor)\u001b[0m\n\u001b[1;32m     46\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mnext_possible_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mposs_states\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                         \u001b[0;31m# need to get the action, probability and reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                         \u001b[0mmove_from_policy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrotation_from_policy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPolicy_Matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;31m#                         if move_from_policy == 0:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m#                             move = 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-b6c021059e7d>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, given_policy)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;31m# populate down matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m                     \u001b[0;31m# determine rotation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# goal on left\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pi_0 = Policy_Matrix()\n",
    "current_state = State(1,4,6)\n",
    "discount_factor = 0.9\n",
    "maxed_value, maxed_policy = policy_iteration(pi_0, discount_factor)\n",
    "full_trajectory = compute_trajectory(maxed_policy, current_state, 0)\n",
    "plt.xlim(0,6)\n",
    "plt.ylim(0,6)\n",
    "for i in range(len(full_trajectory)-1):\n",
    "#     pass\n",
    "    x,y = full_trajectory[i]\n",
    "    x_one_up, y_one_up = full_trajectory[i+1]\n",
    "    plt.arrow(x,y,x_one_up-x,y_one_up-y, head_width=0.1, head_length=0.1,fc='k', ec='k')\n",
    "# plt.arrow(1, 4, 0, -1, head_width=0.05, head_length=0.1, fc='k', ec='k')\n",
    "plt.grid(color='r', linestyle='-', linewidth=0.25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4a:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = [(-1, -1), (-1, 0), (-1, 1), (0, 0), (1, -1), (1, 0), (1, 1)]\n",
    "\n",
    "\n",
    "def value_iteration(discount):\n",
    "    prev_V = np.zeros((12, 6, 6)) # initial value matrix\n",
    "    new_pol_mat = [[[None for l in range(6)] for w in range(6)] for h in range(12)] # new policy matrix\n",
    "    err_p = 0 # error probability\n",
    "    conv = 0 # convergence boolean\n",
    "    \n",
    "    while (conv != 1):\n",
    "        new_V = np.zeros((12, 6, 6)) # new value matrix\n",
    "        mdp = MarkovDecisionProcess(6,6)\n",
    "        for x_pos in range(6):\n",
    "            for y_pos in range(6):\n",
    "                for heading in range(12):\n",
    "                    current_state = State(x_pos, y_pos, heading)\n",
    "                    poss_states = calc_adj_states(current_state)\n",
    "                    best_action = None\n",
    "#                     print(heading, x_pos, y_pos)\n",
    "                    max_action_val = float('-inf')\n",
    "                    for act in action_space:\n",
    "                        action = Action(act[0], act[1]) # create action object for transition probability calculation\n",
    "                        action_val = 0\n",
    "                        for next_state in poss_states:\n",
    "                            x_, y_, h_ = next_state.return_current_state()\n",
    "                            action_val += mdp.transition_probabilities(err_p, action, current_state, next_state) * (mdp.reward(current_state) + discount*prev_V[h_][x_][y_])\n",
    "                        if (action_val > max_action_val):\n",
    "                            max_action_val = action_val\n",
    "                            best_action = action\n",
    "                    # update policy matrix and new value matrix\n",
    "                    new_pol_mat[heading][x_pos][y_pos] = best_action.return_action()\n",
    "                    new_V[heading][x_pos][y_pos] = max_action_val\n",
    "        \n",
    "        # check if convergence occurs\n",
    "#         print(np.sum(np.abs(new_V - prev_V)))\n",
    "        if(np.array_equal(new_V, prev_V)):\n",
    "            conv = 1\n",
    "        # if not, update value matrix\n",
    "#         print('didnt converge')\n",
    "        prev_V = new_V\n",
    "    # create final policy\n",
    "    new_pol = Policy_Matrix(new_pol_mat)\n",
    "    return new_pol, new_V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# testing value iteration function\n",
    "discount = 0.9\n",
    "opt_pol, opt_val = value_iteration(discount)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[(-1, -1), (-1, -1), (-1, -1), (-1, -1), (-1, -1), (-1, -1)], [(1, -1), (1, -1), (-1, -1), (-1, -1), (-1, -1), (-1, -1)], [(1, -1), (0, 0), (-1, -1), (-1, -1), (-1, -1), (-1, -1)], [(1, -1), (1, -1), (1, -1), (1, -1), (0, 0), (-1, -1)], [(1, -1), (0, 0), (-1, -1), (-1, -1), (-1, -1), (-1, -1)], [(-1, -1), (-1, -1), (1, -1), (-1, -1), (-1, -1), (1, -1)]], [[(1, 1), (1, 1), (-1, 1), (-1, 1), (-1, 1), (-1, 1)], [(1, 1), (1, 0), (-1, 1), (-1, 0), (-1, -1), (-1, -1)], [(1, 1), (0, 0), (-1, 1), (1, 1), (-1, 1), (-1, 1)], [(1, -1), (1, -1), (1, -1), (1, -1), (0, 0), (-1, -1)], [(1, 1), (0, 0), (-1, 1), (1, 1), (-1, 1), (-1, 1)], [(1, 1), (1, 1), (-1, 1), (1, 1), (-1, 1), (-1, 1)]], [[(1, -1), (1, 0), (1, -1), (1, -1), (1, -1), (1, -1)], [(1, -1), (1, 0), (0, 0), (0, 0), (0, 0), (1, -1)], [(1, -1), (1, -1), (1, -1), (1, -1), (1, -1), (1, -1)], [(-1, -1), (-1, 0), (0, 0), (0, 0), (0, 0), (-1, -1)], [(-1, -1), (-1, -1), (-1, -1), (-1, -1), (-1, -1), (-1, -1)], [(-1, -1), (-1, 0), (-1, 0), (-1, 0), (-1, 0), (-1, -1)]], [[(1, -1), (1, -1), (1, -1), (1, -1), (1, -1), (-1, -1)], [(1, -1), (1, -1), (0, 0), (0, 0), (0, 0), (1, -1)], [(-1, -1), (-1, -1), (-1, -1), (-1, -1), (1, -1), (-1, -1)], [(-1, -1), (-1, -1), (0, 0), (0, 0), (0, 0), (-1, -1)], [(-1, -1), (-1, -1), (-1, -1), (-1, -1), (-1, -1), (-1, -1)], [(-1, -1), (-1, -1), (-1, -1), (-1, -1), (-1, -1), (-1, -1)]], [[(1, 1), (1, -1), (1, 1), (1, 1), (1, 1), (1, 1)], [(1, 1), (1, 0), (0, 0), (0, 0), (0, 0), (1, 1)], [(1, 1), (1, 1), (1, 1), (1, 1), (1, -1), (1, 1)], [(-1, 1), (-1, 0), (0, 0), (0, 0), (0, 0), (-1, 1)], [(-1, 1), (-1, 1), (-1, 1), (-1, 1), (-1, -1), (-1, 1)], [(-1, 1), (-1, 0), (-1, 0), (-1, 0), (-1, -1), (-1, 1)]], [[(-1, -1), (-1, -1), (1, -1), (1, -1), (1, -1), (1, -1)], [(-1, -1), (-1, 0), (1, -1), (1, 0), (1, 0), (1, 0)], [(-1, -1), (0, 0), (1, -1), (-1, -1), (1, -1), (1, -1)], [(-1, 0), (-1, 0), (-1, 0), (-1, -1), (0, 0), (1, -1)], [(-1, -1), (0, 0), (1, -1), (-1, -1), (1, -1), (1, -1)], [(-1, -1), (-1, -1), (1, -1), (-1, -1), (1, -1), (1, -1)]], [[(1, -1), (-1, -1), (-1, -1), (1, -1), (1, -1), (1, -1)], [(-1, -1), (-1, -1), (-1, -1), (1, -1), (1, -1), (1, -1)], [(-1, -1), (0, 0), (1, -1), (1, -1), (1, -1), (1, -1)], [(-1, -1), (-1, -1), (-1, -1), (-1, -1), (0, 0), (1, -1)], [(-1, -1), (0, 0), (1, -1), (1, -1), (1, -1), (1, -1)], [(1, -1), (-1, -1), (-1, -1), (1, -1), (-1, -1), (-1, -1)]], [[(-1, 1), (-1, 1), (1, 1), (1, 1), (1, 1), (1, 1)], [(-1, 1), (-1, 0), (1, 1), (1, 0), (1, -1), (1, -1)], [(-1, 1), (0, 0), (1, 1), (-1, 1), (1, 1), (1, 1)], [(-1, -1), (-1, -1), (-1, -1), (-1, -1), (0, 0), (1, -1)], [(-1, 1), (0, 0), (1, 1), (-1, 1), (1, 1), (1, 1)], [(-1, 1), (-1, 1), (1, 1), (-1, 1), (1, 1), (1, 1)]], [[(-1, -1), (-1, 0), (-1, -1), (-1, -1), (-1, -1), (-1, -1)], [(-1, -1), (-1, 0), (0, 0), (0, 0), (0, 0), (-1, -1)], [(-1, -1), (-1, -1), (-1, -1), (-1, -1), (-1, -1), (-1, -1)], [(-1, -1), (-1, 0), (0, 0), (0, 0), (0, 0), (-1, -1)], [(1, -1), (1, -1), (1, -1), (1, -1), (1, -1), (1, -1)], [(1, -1), (1, 0), (1, 0), (1, 0), (1, 0), (1, -1)]], [[(-1, -1), (-1, -1), (-1, -1), (-1, -1), (-1, -1), (1, -1)], [(-1, -1), (-1, -1), (0, 0), (0, 0), (0, 0), (-1, -1)], [(-1, -1), (-1, -1), (-1, -1), (-1, -1), (-1, -1), (-1, -1)], [(-1, -1), (-1, -1), (0, 0), (0, 0), (0, 0), (-1, -1)], [(-1, -1), (1, -1), (1, -1), (1, -1), (1, -1), (-1, -1)], [(1, -1), (1, -1), (1, -1), (1, -1), (1, -1), (1, -1)]], [[(-1, 1), (-1, -1), (-1, 1), (-1, 1), (-1, 1), (-1, 1)], [(-1, 1), (-1, 0), (0, 0), (0, 0), (0, 0), (-1, 1)], [(-1, 1), (-1, 1), (-1, 1), (-1, 1), (-1, -1), (-1, 1)], [(-1, 1), (-1, 0), (0, 0), (0, 0), (0, 0), (-1, 1)], [(1, 1), (1, 1), (1, 1), (1, 1), (1, -1), (1, 1)], [(1, 1), (1, 0), (1, 0), (1, 0), (1, -1), (1, 1)]], [[(1, -1), (1, -1), (-1, -1), (-1, -1), (-1, -1), (-1, -1)], [(1, -1), (1, 0), (-1, -1), (-1, 0), (-1, 0), (-1, 0)], [(1, -1), (0, 0), (-1, -1), (1, -1), (-1, -1), (-1, -1)], [(1, 0), (1, 0), (1, 0), (1, -1), (0, 0), (-1, -1)], [(1, -1), (0, 0), (-1, -1), (1, -1), (-1, -1), (-1, -1)], [(1, -1), (1, -1), (-1, -1), (1, -1), (-1, -1), (-1, -1)]]]\n",
      "[[[-266.6953279  -266.6953279  -267.12579511 -266.6953279  -267.12579511\n",
      "   -267.5132156 ]\n",
      "  [ -95.6953279     4.782969      4.3046721     4.782969      4.3046721\n",
      "    -96.12579511]\n",
      "  [-100.            0.          -10.          -13.68559     -19.81       -110.539     ]\n",
      "  [ -93.439         7.29          8.1           9.           10.          -91.        ]\n",
      "  [-100.            0.          -10.          -13.68559     -19.81       -110.539     ]\n",
      "  [-266.217031   -266.217031   -271.729      -266.217031   -271.729\n",
      "   -271.729     ]]\n",
      "\n",
      " [[-185.217031   -185.6953279  -185.217031   -185.6953279  -186.12579511\n",
      "   -186.5132156 ]\n",
      "  [ -94.68559       4.782969      5.31441       4.782969      4.3046721\n",
      "    -96.12579511]\n",
      "  [ -94.0951        0.           -4.0951      -10.9         -11.71       -100.9       ]\n",
      "  [ -93.439         7.29          8.1           9.           10.          -91.        ]\n",
      "  [ -94.0951        0.           -4.0951      -10.9         -11.71       -100.9       ]\n",
      "  [-184.68559    -192.1951     -184.68559    -190.81       -191.539      -190.81      ]]\n",
      "\n",
      " [[-185.217031    -94.68559     -95.217031    -95.6953279   -96.12579511\n",
      "   -186.5132156 ]\n",
      "  [-184.68559       5.9049        0.            0.            0.         -190.81      ]\n",
      "  [-184.0951        6.561        -2.71         -1.9          -1.         -181.9       ]\n",
      "  [-184.68559       5.9049        0.            0.           10.         -190.81      ]\n",
      "  [-184.0951        6.561        -2.71         -1.9          -1.         -181.9       ]\n",
      "  [-184.68559     -94.0951     -102.439      -101.71       -100.9        -190.81      ]]\n",
      "\n",
      " [[-266.217031    -94.68559    -100.         -100.         -100.\n",
      "   -267.86189404]\n",
      "  [-265.68559       5.9049        0.            0.            0.         -263.71      ]\n",
      "  [-266.217031      5.31441     -10.          -10.           -1.         -271.729     ]\n",
      "  [-265.68559       5.9049        0.            0.           10.         -263.71      ]\n",
      "  [-266.217031      5.31441     -10.          -10.           -1.         -271.729     ]\n",
      "  [-265.68559     -94.0951     -102.439      -101.71       -100.9        -263.71      ]]\n",
      "\n",
      " [[-185.217031    -94.68559     -95.217031    -95.6953279   -96.12579511\n",
      "   -186.5132156 ]\n",
      "  [-184.68559       5.9049        0.            0.            0.         -190.81      ]\n",
      "  [-184.0951        6.561        -2.71         -1.9          -1.         -181.9       ]\n",
      "  [-184.68559       5.9049        0.            0.           10.         -190.81      ]\n",
      "  [-184.0951        6.561        -2.71         -1.9          -1.         -181.9       ]\n",
      "  [-184.68559     -94.0951     -102.439      -101.71       -100.9        -190.81      ]]\n",
      "\n",
      " [[-185.217031   -185.6953279  -185.217031   -185.6953279  -186.12579511\n",
      "   -186.5132156 ]\n",
      "  [ -94.68559       4.782969      5.31441       4.782969      4.3046721\n",
      "    -96.12579511]\n",
      "  [ -94.0951        0.           -4.0951      -10.9         -11.71       -100.9       ]\n",
      "  [ -93.439         7.29          8.1           9.           10.          -91.        ]\n",
      "  [ -94.0951        0.           -4.0951      -10.9         -11.71       -100.9       ]\n",
      "  [-184.68559    -192.1951     -184.68559    -190.81       -191.539      -190.81      ]]\n",
      "\n",
      " [[-266.6953279  -266.6953279  -267.12579511 -266.6953279  -267.12579511\n",
      "   -267.5132156 ]\n",
      "  [ -95.6953279     4.782969      4.3046721     4.782969      4.3046721\n",
      "    -96.12579511]\n",
      "  [-100.            0.          -10.          -13.68559     -19.81       -110.539     ]\n",
      "  [ -93.439         7.29          8.1           9.           10.          -91.        ]\n",
      "  [-100.            0.          -10.          -13.68559     -19.81       -110.539     ]\n",
      "  [-266.217031   -266.217031   -271.729      -266.217031   -271.729\n",
      "   -271.729     ]]\n",
      "\n",
      " [[-185.217031   -185.6953279  -185.217031   -185.6953279  -186.12579511\n",
      "   -186.5132156 ]\n",
      "  [ -94.68559       4.782969      5.31441       4.782969      4.3046721\n",
      "    -96.12579511]\n",
      "  [ -94.0951        0.           -4.0951      -10.9         -11.71       -100.9       ]\n",
      "  [ -93.439         7.29          8.1           9.           10.          -91.        ]\n",
      "  [ -94.0951        0.           -4.0951      -10.9         -11.71       -100.9       ]\n",
      "  [-184.68559    -192.1951     -184.68559    -190.81       -191.539      -190.81      ]]\n",
      "\n",
      " [[-185.217031    -94.68559     -95.217031    -95.6953279   -96.12579511\n",
      "   -186.5132156 ]\n",
      "  [-184.68559       5.9049        0.            0.            0.         -190.81      ]\n",
      "  [-184.0951        6.561        -2.71         -1.9          -1.         -181.9       ]\n",
      "  [-184.68559       5.9049        0.            0.           10.         -190.81      ]\n",
      "  [-184.0951        6.561        -2.71         -1.9          -1.         -181.9       ]\n",
      "  [-184.68559     -94.0951     -102.439      -101.71       -100.9        -190.81      ]]\n",
      "\n",
      " [[-266.217031    -94.68559    -100.         -100.         -100.\n",
      "   -267.86189404]\n",
      "  [-265.68559       5.9049        0.            0.            0.         -263.71      ]\n",
      "  [-266.217031      5.31441     -10.          -10.           -1.         -271.729     ]\n",
      "  [-265.68559       5.9049        0.            0.           10.         -263.71      ]\n",
      "  [-266.217031      5.31441     -10.          -10.           -1.         -271.729     ]\n",
      "  [-265.68559     -94.0951     -102.439      -101.71       -100.9        -263.71      ]]\n",
      "\n",
      " [[-185.217031    -94.68559     -95.217031    -95.6953279   -96.12579511\n",
      "   -186.5132156 ]\n",
      "  [-184.68559       5.9049        0.            0.            0.         -190.81      ]\n",
      "  [-184.0951        6.561        -2.71         -1.9          -1.         -181.9       ]\n",
      "  [-184.68559       5.9049        0.            0.           10.         -190.81      ]\n",
      "  [-184.0951        6.561        -2.71         -1.9          -1.         -181.9       ]\n",
      "  [-184.68559     -94.0951     -102.439      -101.71       -100.9        -190.81      ]]\n",
      "\n",
      " [[-185.217031   -185.6953279  -185.217031   -185.6953279  -186.12579511\n",
      "   -186.5132156 ]\n",
      "  [ -94.68559       4.782969      5.31441       4.782969      4.3046721\n",
      "    -96.12579511]\n",
      "  [ -94.0951        0.           -4.0951      -10.9         -11.71       -100.9       ]\n",
      "  [ -93.439         7.29          8.1           9.           10.          -91.        ]\n",
      "  [ -94.0951        0.           -4.0951      -10.9         -11.71       -100.9       ]\n",
      "  [-184.68559    -192.1951     -184.68559    -190.81       -191.539      -190.81      ]]]\n"
     ]
    }
   ],
   "source": [
    "print(opt_pol.pol_mat)\n",
    "print(opt_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcbc71fa050>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "current_state = State(1,4,6)\n",
    "discount_factor = 0.9\n",
    "full_trajectory = compute_trajectory(opt_pol, current_state, 0)\n",
    "plt.xlim(0,6)\n",
    "plt.ylim(0,6)\n",
    "for i in range(len(full_trajectory)-1):\n",
    "#     pass\n",
    "    x,y = full_trajectory[i]\n",
    "    x_one_up, y_one_up = full_trajectory[i+1]\n",
    "    plt.arrow(x,y,x_one_up-x,y_one_up-y, head_width=0.1, head_length=0.1,fc='k', ec='k')\n",
    "# plt.arrow(1, 4, 0, -1, head_width=0.05, head_length=0.1, fc='k', ec='k')\n",
    "plt.grid(color='r', linestyle='-', linewidth=0.25)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
